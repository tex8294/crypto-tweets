{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project aims to analyze twitter messages with keywords related to bitcoin in order to make a guess on the movement of the price for the next day. To do this, we scraped twitter for a small set of tweets with the keywords bitcoin, cryptocurrency, and BTC. Unfortunately, there weren't any recent premade datasets we could reference for training. The twitter API was also very restrictive as well so our data set ended up being a limiting factor for how well we could train our final network. \n",
    "\n",
    "We take these tweets, do a sentiment analysis with a pretrained activation network/VADER api. Then, we pass vectors with the sentiments for each day along with the price movement of the following day as training data. the idea here is to train the network to take the day's sentiment and use it as a guide to guess whether the prices the next day do indeed rise or fall. The validation data would be the actual price movements of bitcoin with the same granularity as our (limited) twitter dataset. \n",
    "\n",
    "To start the code, we scrape twitter using the Twipy API and output the results into a formatted csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parse and clean the data in the tables to prepare it for the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pass it through the text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we put the sentiment analysis in blocks consisting of one day's worth of tweet sentiments. We then pair this with the next day's price as the \"label\" and feed these into a sequential keras instance. Reserving some data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a feed-forward network with an activation layer and a dense layer. we used a mean square error and binary cross entropy for our loss function.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
