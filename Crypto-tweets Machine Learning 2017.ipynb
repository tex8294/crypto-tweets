{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project aims to analyze twitter messages with keywords related to bitcoin in order to make a guess on the movement of the price for the next day. To do this, we scraped twitter for a small set of tweets with the keywords bitcoin, cryptocurrency, and BTC. Unfortunately, there weren't any recent premade datasets we could reference for training. The twitter API was also very restrictive as well so our data set ended up being a limiting factor for how well we could train our final network. We searched for online databases, but they all were for years before bitcoin had an active market. \n",
    "\n",
    "We take these tweets, do a sentiment analysis with a logistical regresion method. Then, we pass vectors with the sentiments for each day along with the price movement of the following day as training data. The idea here is to train the network to take the day's sentiment and use it as a guide to guess whether the prices the next day do indeed rise or fall. \n",
    "\n",
    "To start the code, we scrape twitter using the Twipy API and output the results into a formatted csv file:\n",
    "\n",
    "We define a function titled \"twitter_search\" which uses the tweepy library to search up to 10 days in the past for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_search(authentication, path, start_date, final_date, word):\n",
    "#Authentication is a list cointaining the setup parameters (including a secret key)\n",
    "    \n",
    "    import tweepy\n",
    "    import csv\n",
    "    import re\n",
    "    import time\n",
    "    \n",
    "    #autentication parameters\n",
    "    access_token = authentication[0]\n",
    "    access_token_secret = authentication[1]\n",
    "    consumer_key = authentication[2]\n",
    "    consumer_secret = authentication[3]\n",
    "    \n",
    "    # autentication\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then limit the number of api calls we make to get a broader range of tweets before tripping the API limit, then pass the tweets into a csv file while parsing the text and cleaning up any replies or retweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-638ab69c1110>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-638ab69c1110>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    csvFile = open(path, 'a')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " # Open/Create a file to append data\n",
    "csvFile = open(path, 'a')\n",
    "    \n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile, delimiter='\\t' )\n",
    "   \n",
    "cont = 1;\n",
    "#Tweet searching\n",
    "current_date = final_date;\n",
    "while current_date !=start_date:\n",
    "     try:\n",
    "        for tweet in tweepy.Cursor(api.search,\n",
    "                                       q=word,\n",
    "                                       since_id=start_date,\n",
    "                                       until=current_date,\n",
    "                                       count=\"1000\",\n",
    "                                       lang = 'en').items():\n",
    "            if  ('RT @' not in tweet.text): #elimination of the retweets\n",
    "                   tweet.text = re.sub(r\"http\\S+\",\"\",tweet.text); #elimination of the URLs in tweets\n",
    "                   #print('\\nDate: ', tweet.created_at, '\\nAuthor of the tweet: ', tweet.user.name, '\\n', tweet.text, '\\nNumber of retweets: ', tweet.retweet_count)\n",
    "                   csvWriter.writerow([tweet.retweet_count, tweet.user.name.encode('utf-8'), tweet.created_at, tweet.text.encode('utf-8')])\n",
    "                   cont = cont +1;\n",
    "            if(cont == 1000000):\n",
    "                    current_date = time_decrease(current_date)\n",
    "                    cont = 0\n",
    "                    break\n",
    "            except tweepy.TweepError:\n",
    "            pass\n",
    "     \n",
    "def time_decrease(date):\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    date = datetime.strptime( date, '%Y-%m-%d');\n",
    "    delta = timedelta(days=1);\n",
    "    new_date = date-delta;\n",
    "    final_date = datetime.isoformat(new_date)[0:10];\n",
    "    return final_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass the twitter csv data through the text classifier. We tried both linear and non-linear SVMs, decision trees, logistic regression, and Multi-layer perceptrons. We settled on the effectiveness (~65-70% accuracy)of the MLP in the end. For this first block of code, we import our dependencies, drawing from the scikit learn library among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-507aa8510ced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "dim_e=100 #dimension of the embedding space\n",
    "\n",
    "n_m=100000     #number of mails \n",
    "\n",
    "v_dim=10000  #validation set dimension\n",
    "\n",
    "#read csv file\n",
    "\n",
    "def read_csv(filename,n_m):\n",
    "  with open(filename) as csvfile:         \n",
    "      readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "      values = []\n",
    "      text = []\n",
    "      for row in readCSV:\n",
    "        \n",
    "          a=float(row[1])\n",
    "          b = row[3]\n",
    "        \n",
    "          values.append(a)\n",
    "          text.append(b)\n",
    "\n",
    "  return values[0:n_m],text[0:n_m]\n",
    "\n",
    "values,mails=read_csv('sent3.csv',n_m)\n",
    "labels=np.asarray(values)\n",
    "mails0=mails[0:n_m-v_dim]  #we don't take the validation set for training \n",
    "text_mails=[s for sentence in mails0 for s in sent_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the sample.txt file containing words from many novels on the project Gutenburg literature databse.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('sample.txt')  #read the txt file\n",
    "text0=f.read()\n",
    "text1=text_mails +sent_tokenize(text0)\n",
    "#text1=text_mails \n",
    "\n",
    "#tokenize, remove punctuation, common words and very rare words\n",
    "stoplist = set('for a of the and to in ! \" # $ % & ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~'.split())\n",
    "texts = [[word.lower() for word in word_tokenize(document) if word not in stoplist] for document in text1]\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model using the gensim library and its built-in word-to-vec capability to format our data for the network. After training, we sort the vectors by frequency to ensure the word-2-vec conversion worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(texts, min_count=5, iter=20, size=dim_e, sorted_vocab=1)\n",
    "#use the model to calculate similiarities between words\n",
    "\n",
    "#sort vectors by frequency\n",
    "\n",
    "l_k=list(model.wv.vocab.keys())\n",
    "\n",
    "def f_key(x):\n",
    "    return model.wv.vocab[x].count\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "   \n",
    "label=list(reversed(sorted(l_k,key=f_key)))\n",
    "\n",
    "X_sort=X\n",
    "for j in range(0,len(label)):\n",
    "    X_sort[j,:]=model[label[j]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then visualize the effectiveness of the word-to-vec encoding with a t-SNE plot to see how words are inter-related and how closely the machine pairs similar words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tsne visualize\n",
    "plot_i=0\n",
    "plot_f=500\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X_sort[plot_i:plot_f,:])\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "  plt.show()\n",
    "\n",
    "plot_with_labels(X_tsne, label[plot_i:plot_f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import bitcoin pricing data to be loaded later and mixed with our tweet sentiments. This function pairs a tweet with a price set a couple hours after the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(date1, date2):\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    date1 = datetime.strptime( date1, '%Y-%m-%d %H:%M');\n",
    "    date2 = datetime.strptime( date2, '%Y-%m-%d %H:%M');\n",
    "    return date1 < date2\n",
    "    \n",
    "def time_decrease(date):\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    date = datetime.strptime( date, '%Y-%m-%d');\n",
    "    delta = timedelta(days=1);\n",
    "    new_date = date-delta;\n",
    "    new_date = date-delta;\n",
    "    final_date = datetime.isoformat(new_date)[0:10];\n",
    "    return final_date\n",
    "\n",
    "def convert(date):\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    import time\n",
    "    new_date = time.mktime(datetime.strptime(date, '%Y-%m-%d %H:%M').timetuple())\n",
    "    return new_date\n",
    "\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('C:\\\\Users\\\\Marco\\\\Desktop\\\\name.csv', 'a')\n",
    "    \n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile, delimiter='\\t' )\n",
    "\n",
    "\n",
    "with open('C:\\\\Users\\\\Marco\\\\Desktop\\\\tweets.csv') as f:\n",
    "    r = csv.reader(f, delimiter=';')\n",
    "    r = list(r)\n",
    "    \n",
    "with open('C:\\\\Users\\\\Marco\\\\Desktop\\\\val.csv') as f:\n",
    "    s = csv.reader(f, delimiter=';')\n",
    "    s = list(s)\n",
    "\n",
    "            \n",
    "xp=np.zeros(len(s))\n",
    "yp=np.zeros(len(s))\n",
    "cont = 0\n",
    "for row in s:\n",
    "    xp[cont]=convert(row[0])\n",
    "    yp[cont]=float(row[1])\n",
    "    cont = cont + 1\n",
    "\n",
    "x=np.zeros(len(r))\n",
    "cont = 0\n",
    "for row in r:\n",
    "    x[cont]=convert(row[2])\n",
    "    cont = cont + 1\n",
    "\n",
    "y=np.zeros(len(r))\n",
    "y=np.interp(x,xp,yp)\n",
    "\n",
    "cont = 0\n",
    "for row in r:\n",
    "   csvWriter.writerow([row[0],row[2],row[3],y[cont]])\n",
    "   cont = cont+1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we commence with the business end of the neural network which uses an MLP to categorize words from the tweets and then pairs them into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1c96bc583be7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext_mails0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmail\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmails\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtext_mails\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_mails0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_m\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mails' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "text_mails0=[[s  for s in sent_tokenize(mail)] for mail in mails]\n",
    "text_mails=text_mails0[0:n_m]\n",
    "\n",
    "words = [[word.lower() for sentence in mail for word in word_tokenize(sentence) if word not in stoplist] for mail in text_mails]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in words:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "words = [[token for token in text if frequency[token] > 1] for text in words]\n",
    "\n",
    "#sum of the word vectors for each mail\n",
    "\n",
    "mail_vectors=np.zeros((n_m,dim_e)); \n",
    "\n",
    "for j in range(0,n_m):\n",
    "    m=words[j]\n",
    "    for k in range(0,len(m)):\n",
    "      try:\n",
    "        mail_vectors[j,:]=mail_vectors[j,:]+np.asarray(model[m[k]]) \n",
    "      except KeyError:\n",
    "        pass #skips words which aren't in the learned vocabulary\n",
    "        \n",
    "clf=MLPClassifier(alpha=1)    #0.248 with sample and 200000\n",
    "\n",
    "#to train the model we only take the first n_m-v_dim mails (the training set)\n",
    "print('training')\n",
    "clf.fit(mail_vectors[0:n_m-v_dim],labels[0:n_m-v_dim])\n",
    "\n",
    "#output to monitor validation error\n",
    "\n",
    "print(\"validation error=\",np.sum(np.abs(labels[n_m-v_dim:n_m]-clf.predict(mail_vectors[n_m-v_dim:n_m,:])))/v_dim)\n",
    "\n",
    "print(\"s=\",clf.predict(mail_vectors[1:100,:]))\n",
    "print(\"l=\",labels[1:100])\n",
    "def read_csv1(filename):\n",
    "  with open(filename) as csvfile:         \n",
    "      readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "      text = []\n",
    "      price = []\n",
    "      for row in readCSV:\n",
    "        a=row[2]\n",
    "        b=row[3]\n",
    "        text.append(a)\n",
    "        price.append(b)\n",
    "        \n",
    "  return text, price\n",
    "\n",
    "tweets, price =read_csv1('tweet-price.csv') #time is an array with the day for each tweet \n",
    "\n",
    "text_mails=[[s  for s in sent_tokenize(mail)] for mail in tweets]\n",
    "\n",
    "\n",
    "#calculate the sentiments and word vectors for each tweet\n",
    "\n",
    "words = [[word.lower() for sentence in mail for word in word_tokenize(sentence) if word not in stoplist] for mail in text_mails]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in words:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "words = [[token for token in text if frequency[token] > 1] for text in words]\n",
    "\n",
    "tweet_vectors=np.zeros((len(words),dim_e));\n",
    "\n",
    "tweet_sentiment=np.zeros(len(words))\n",
    "\n",
    "\n",
    "\n",
    "for j in range(0,len(words)):\n",
    "    m=words[j]                  #word vectors for each tweet\n",
    "    for k in range(0,len(m)):\n",
    "      try:                    \n",
    "        tweet_vectors[j,:]=tweet_vectors[j,:]+np.asarray(model[m[k]])\n",
    "      except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "tweet_sentiment=clf.predict(tweet_vectors);   #sentiments\n",
    "\n",
    "print('sentiments=',tweet_sentiment)\n",
    "\n",
    "X_train=np.asarray(tweet_sentiment)\n",
    "\n",
    "Y_train=np.asarray(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we put the sentiment analysis in blocks consisting of tweet sentiments. We then pair each tweet sentiment with an associated bitcoin price set a couple hours after the tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the tweets and prices paired with the sentiments, we put them in a csv file for more portability then converted the two columns of the csv into arrays. the arrays get fed into a binary classifier to try to distinguish between up and down \"guesses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import TFOptimizer\n",
    "import numpy as np\n",
    "\n",
    "coinarrays=[]\n",
    "coinarrayp=[]\n",
    "with open('puta2.csv', newline='') as coinfile:\n",
    "    coinreader=csv.reader(coinfile, delimiter='\\t', quotechar='|')\n",
    "    for row in coinreader:\n",
    "        coinarrays.append(float(row[0]))\n",
    "        coinarrayp.append(float(row[1]))\n",
    "\n",
    "coinlen=len(coinarrays)-801\n",
    "       \n",
    "\n",
    "x_train = np.transpose(coinarrays[801:])\n",
    "y_train = np.transpose(coinarrayp[801:])\n",
    "x_test = coinarrays[0:800]\n",
    "y_test = coinarrayp[0:800]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=800)\n",
    "score = model.evaluate(x_test, y_test, batch_size=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mse error and loss functions ended up being extremely high for our data. The sentiment analysis operated on a binary basis, giving our data a vey high granularity while the exponential increase of bitcoin price within the limited window we were able to obtain our tweets means that network optimization aside, our results were somewhat doomed from the start to be very skewed.\n",
    "However, as an exercise this project taught us a lot about the large amounts of effort and time machine learning and data science in general require. We admit defeat to the twitter datalords."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
